DATA:
  local_prefix: dataset/
  data_root: cityscapes
  train_labeled_list: cityscapes/list/cs_split8_labeled.txt
  train_unlabeled_list: cityscapes/list/cs_split8_unlabeled.txt
  prediction_list: cityscapes/list/cs_split8_unlabeled.txt
  val_list: cityscapes/list/cs_val.txt
  test_list: cityscapes/list/cs_val.txt
  classes: 19

TRAIN:
  arch: psp
  layers: 50
  sync_bn: True  # adopt syncbn or not
  train_h: 361
  train_w: 361
  scale_min: 0.25  # minimum random scale
  scale_max: 1.0  # maximum random scale
  rotate_min: -10  # minimum random rotate
  rotate_max: 10  # maximum random rotate
  zoom_factor: 8  # zoom factor for final prediction during training, be in [1, 2, 4, 8]
  ignore_label: 255
  aux_weight: 0.4
  train_gpu: [4]
  workers: 0  # data loader workers
  batch_size: 1  # batch size for training
  unlabelled_batch_size: 4
  batch_size_val: 8  # batch size for validation during training, memory and speed tradeoff
  base_lr: 0.01
  epochs: 1
  start_epoch: 0
  power: 0.9
  momentum: 0.9
  weight_decay: 0.0001
  manual_seed: 1
  print_freq: 1
  save_freq: 1
  save_path: exp/
  weight: initmodel/train_psp50_cityscapes_split8_crop361_round1.pth   # path to initial weight (default: none)
  resume:  #exp/ex_test/model/train_epoch_280.pth # path to latest checkpoint (default: none)
  evaluate: False  # evaluate on validation set, extra gpu memory needed and small batch_size_val is recommend
  evaluate_start: 0
  evaluate_freq: 20
  evaluate_full_size: True
  evaluate_previous_best_val_mIou: 0.63

  sup_loss_method: 'CE'   # 'MSE' 'CE' 'lovasz'
  consistency_lambda: 1.0

  save_pseudo_label_path: cityscapes/pseudo_labels/psp50_cityscapes_split8_crop361_for_round2/
  npy_describ: 'npy_psp50_cityscapes_split8_crop361_for_round2'
  global_num: 2603
  semi_split: '8'
  pseudo_data: cityscapes
  update_pseudo_label: True
  update_pseudo_start: 0
  update_pseudo_freq: 1
  labeling_ratio: 0.5
  thresholds_method: 'DARS'   # cbst or DARS
  save_npy_or_png: 'npy'
  list_write: 'img_pse'  #'pse_gt'  #
  diagram_T: 1.0
  temp_scaling: False
  ###------------------------------------------------ my added config

Distributed:
  dist_url: tcp://127.0.0.1:6787
  dist_backend: 'nccl'
  multiprocessing_distributed: True
  world_size: 1
  rank: 0
  use_apex: True
  opt_level: 'O0'
  keep_batchnorm_fp32:
  loss_scale:

TEST:
  split: val  # split in [train, val and test]
#  split: test  # split in [train, val and test]
  base_size: 1024  # based size for scaling
#  base_size: 2048  # based size for scaling
#  test_h: 713
  test_h: 361
#  test_w: 713
  test_w: 361
  scales: [1.0]  # evaluation scales, ms as [0.5, 0.75, 1.0, 1.25, 1.5, 1.75]
#  scales: [0.5, 0.75, 1.0, 1.25, 1.5, 1.75]
  has_prediction: False  # has prediction already or not
  index_start: 0  # evaluation start index in list
  index_step: 0  # evaluation step index in list, 0 means to end
  test_gpu: [0]
  test_adabn: False
  colors_path: data/cityscapes/cityscapes_colors.txt  # path of dataset colors
  names_path: data/cityscapes/cityscapes_names.txt  # path of dataset category names